{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b66a11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading categories.xlsx...\n",
      "Splitting hierarchies...\n",
      "Saving to categories_cleaned.xlsx...\n",
      "Done! Here is a preview of the new columns:\n",
      "              Category Level 1  Category Level 2  \\\n",
      "0  Mineral building materials           Binders    \n",
      "1        Insulation materials             Straw    \n",
      "2                      Metals    Steel and iron    \n",
      "3        Insulation materials         Hemp fibre   \n",
      "4        Insulation materials      In‑situ foam    \n",
      "\n",
      "                                Category Level 3 Category Level 4  \n",
      "0                                         Cement             None  \n",
      "1                                    Straw bales             None  \n",
      "2   Cast and forged parts made of steel and iron             None  \n",
      "3                                           None             None  \n",
      "4                                        Mineral             None  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "INPUT_FILE = 'categories.xlsx'     \n",
    "SHEET_NAME = 'Sheet1'                   \n",
    "HIERARCHY_COL = 'Category (en)'         \n",
    "OUTPUT_FILE = 'categories_cleaned.xlsx'\n",
    "\n",
    "def split_hierarchies():\n",
    "    print(f\"Reading {INPUT_FILE}...\")\n",
    "    try:\n",
    "        df = pd.read_excel(INPUT_FILE, sheet_name=SHEET_NAME)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return\n",
    "    \n",
    "    if HIERARCHY_COL not in df.columns:\n",
    "        print(f\"Error: Column '{HIERARCHY_COL}' not found in the Excel file.\")\n",
    "        print(f\"Available columns are: {list(df.columns)}\")\n",
    "        return\n",
    "    print(\"Splitting hierarchies...\")\n",
    "\n",
    "    # 1. Split the column by '/'\n",
    "    # expand=True puts the results into separate columns automatically\n",
    "    split_columns = df[HIERARCHY_COL].str.split('/', expand=True)\n",
    "\n",
    "    # 2. Rename the new columns to Level 1, Level 2, etc.\n",
    "    # This automatically names them based on how many levels were found\n",
    "    num_levels = split_columns.shape[1]\n",
    "    new_col_names = [f\"Category Level {i+1}\" for i in range(num_levels)]\n",
    "    split_columns.columns = new_col_names\n",
    "\n",
    "    # 3. Join the new columns back to the original data\n",
    "    # We drop the original hierarchy column to keep it clean (remove .drop part if you want to keep it)\n",
    "    df_clean = pd.concat([df, split_columns], axis=1)\n",
    "\n",
    "    # 4. Save to Excel\n",
    "    print(f\"Saving to {OUTPUT_FILE}...\")\n",
    "    df_clean.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(\"Done! Here is a preview of the new columns:\")\n",
    "    print(df_clean[new_col_names].head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    split_hierarchies\n",
    "\n",
    "split_hierarchies()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e525a55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\brightway\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING MAPPER ---\n",
      "Loading files...\n",
      "   Using product column: Unnamed: 0\n",
      "Preparing categories...\n",
      "Loading transformer model...\n",
      "Encoding data...\n",
      "Matching...\n",
      "\n",
      "==============================\n",
      "       MAPPING SUMMARY       \n",
      "==============================\n",
      "Total Products Processed: 4186\n",
      "------------------------------\n",
      "✅ Auto-Assigned:  2896  (69.2%)\n",
      "⚠️  Review Needed:  1290  (30.8%)\n",
      "==============================\n",
      "\n",
      "--- Done! Results saved to Mapped_EPD_Results.xlsx ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Make sure these match your actual file names\n",
    "PRODUCT_FILE = 'EPDs_Missing_Categories.xlsx' \n",
    "CATEGORY_FILE = 'categories_cleaned.xlsx'\n",
    "OUTPUT_FILE = 'Mapped_EPD_Results.xlsx'\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.80\n",
    "\n",
    "# --- SMART LOADER: Handles CSV or Excel ---\n",
    "def load_file(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        # Try appending .csv if the user forgot it, or check for \" - Sheet1.csv\" variants\n",
    "        if os.path.exists(filename + \" - Missing Categories.csv\"):\n",
    "            filename = filename + \" - Missing Categories.csv\"\n",
    "        elif os.path.exists(filename + \" - Sheet1.csv\"):\n",
    "            filename = filename + \" - Sheet1.csv\"\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Could not find file: {filename}\")\n",
    "            \n",
    "    if filename.endswith('.csv'):\n",
    "        return pd.read_csv(filename)\n",
    "    elif filename.endswith('.xlsx'):\n",
    "        return pd.read_excel(filename)\n",
    "    else:\n",
    "        raise ValueError(\"File must be .csv or .xlsx\")\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'sku:\\s*\\S+', '', text)\n",
    "    text = re.sub(r'[!@#$%^&*_{};:,.<>?|`~]', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def main():\n",
    "    print(\"--- STARTING MAPPER ---\")\n",
    "\n",
    "    # 1. LOAD FILES\n",
    "    try:\n",
    "        print(\"Loading files...\")\n",
    "        df_prods = load_file(PRODUCT_FILE)\n",
    "        df_cats = load_file(CATEGORY_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. DETECT PRODUCT COLUMN\n",
    "    candidates = ['product name', 'name', 'name (en)', 'flow', 'ref']\n",
    "    prod_col = None\n",
    "    for col in df_prods.columns:\n",
    "        for cand in candidates:\n",
    "            if cand.lower() in col.lower():\n",
    "                prod_col = col\n",
    "                break\n",
    "        if prod_col: break\n",
    "    \n",
    "    if not prod_col:\n",
    "        print(\"Could not find product column. Using first column.\")\n",
    "        prod_col = df_prods.columns[0]\n",
    "    \n",
    "    print(f\"   Using product column: {prod_col}\")\n",
    "\n",
    "    # 3. PREPARE CATEGORIES\n",
    "    print(\"Preparing categories...\")\n",
    "    # Join all columns in the category file to make a path\n",
    "    cat_paths = df_cats.apply(lambda x: ' > '.join(x.dropna().astype(str)), axis=1).tolist()\n",
    "    unique_cats = list(set(cat_paths))\n",
    "\n",
    "    # 4. ENCODE & MATCH\n",
    "    print(\"Loading transformer model...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    print(\"Encoding data...\")\n",
    "    cleaned_names = [clean_text(str(x)) for x in df_prods[prod_col]]\n",
    "    prod_emb = model.encode(cleaned_names, convert_to_tensor=True)\n",
    "    cat_emb = model.encode(unique_cats, convert_to_tensor=True)\n",
    "\n",
    "    print(\"Matching...\")\n",
    "    cosine_scores = util.cos_sim(prod_emb, cat_emb)\n",
    "    top_scores, top_indices = cosine_scores.max(dim=1)\n",
    "\n",
    "    # 5. SAVE RESULTS\n",
    "    results = []\n",
    "    for i, name in enumerate(df_prods[prod_col]):\n",
    "        score = top_scores[i].item()\n",
    "        cat_idx = top_indices[i].item()\n",
    "        \n",
    "        results.append({\n",
    "            \"Original Product\": name,\n",
    "            \"Matched Category\": unique_cats[cat_idx],\n",
    "            \"Confidence\": round(score, 4),\n",
    "            \"Status\": \"Review Needed\" if score < CONFIDENCE_THRESHOLD else \"Auto-Assigned\"\n",
    "        })\n",
    "\n",
    "    df_res = pd.DataFrame(results)\n",
    "\n",
    "    # 6. PRINT STATISTICS (NEW SECTION)\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"       MAPPING SUMMARY       \")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    total_count = len(df_res)\n",
    "    status_counts = df_res['Status'].value_counts()\n",
    "    \n",
    "    auto_count = status_counts.get('Auto-Assigned', 0)\n",
    "    review_count = status_counts.get('Review Needed', 0)\n",
    "    \n",
    "    print(f\"Total Products Processed: {total_count}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"✅ Auto-Assigned:  {auto_count}  ({(auto_count/total_count)*100:.1f}%)\")\n",
    "    print(f\"⚠️  Review Needed:  {review_count}  ({(review_count/total_count)*100:.1f}%)\")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "    # 7. SAVE RESULTS\n",
    "    # Split categories back into levels\n",
    "    split_cats = df_res['Matched Category'].str.split(' > ', expand=True)\n",
    "    split_cats.columns = [f\"Level {i+1}\" for i in range(split_cats.shape[1])]\n",
    "    \n",
    "    final_df = pd.concat([df_res, split_cats], axis=1)\n",
    "    final_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(f\"--- Done! Results saved to {OUTPUT_FILE} ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f64bc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING MAPPER (SentenceTransformer) ---\n",
      "Loading: EPDs_Missing_Categories.xlsx\n",
      "Loading: categories_cleaned.xlsx\n",
      "Using product column: Unnamed: 0\n",
      "Using Authority column: Registration authority\n",
      "Loading model...\n",
      "Encoding data (this may take a moment)...\n",
      "Matching...\n",
      "\n",
      "--- Generating Statistics ---\n",
      "Detailed counts saved to: EPD_Counts_Per_Category_Per_Operator.xlsx\n",
      "Main results saved to: Mapped_EPD_Results.xlsx\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "PRODUCT_FILE = 'EPDs_Missing_Categories.xlsx' \n",
    "CATEGORY_FILE = 'categories_cleaned.xlsx'\n",
    "OUTPUT_FILE = 'Mapped_EPD_Results.xlsx'\n",
    "STATS_FILE = 'Operator_Statistics.xlsx'\n",
    "COUNTS_FILE = 'EPD_Counts_Per_Category_Per_Operator.xlsx'\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.80\n",
    "\n",
    "def load_file(filename):\n",
    "    \"\"\"Smart loader for CSV or Excel.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        # Handle common file name variations\n",
    "        if os.path.exists(filename + \" - Missing Categories.csv\"):\n",
    "            filename = filename + \" - Missing Categories.csv\"\n",
    "        elif os.path.exists(filename + \" - Sheet1.csv\"):\n",
    "            filename = filename + \" - Sheet1.csv\"\n",
    "    \n",
    "    print(f\"Loading: {filename}\")\n",
    "    if filename.endswith('.csv'):\n",
    "        return pd.read_csv(filename)\n",
    "    elif filename.endswith('.xlsx'):\n",
    "        return pd.read_excel(filename)\n",
    "    else:\n",
    "        raise ValueError(\"File must be .csv or .xlsx\")\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'sku:\\s*\\S+', '', text)\n",
    "    text = re.sub(r'[!@#$%^&*_{};:,.<>?|`~]', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def main():\n",
    "    print(\"--- STARTING MAPPER (SentenceTransformer) ---\")\n",
    "\n",
    "    # 1. LOAD FILES\n",
    "    try:\n",
    "        df_prods = load_file(PRODUCT_FILE)\n",
    "        df_cats = load_file(CATEGORY_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. DETECT COLUMNS\n",
    "    # Product Name\n",
    "    candidates = ['product name', 'name', 'name (en)', 'flow', 'ref']\n",
    "    prod_col = None\n",
    "    for col in df_prods.columns:\n",
    "        if any(cand.lower() in col.lower() for cand in candidates):\n",
    "            prod_col = col\n",
    "            break\n",
    "    if not prod_col: prod_col = df_prods.columns[0]\n",
    "    print(f\"Using product column: {prod_col}\")\n",
    "\n",
    "    # Registration Authority\n",
    "    auth_col = None\n",
    "    for col in df_prods.columns:\n",
    "        if \"registration\" in col.lower() and \"authority\" in col.lower():\n",
    "            auth_col = col\n",
    "            break\n",
    "    if auth_col:\n",
    "        print(f\"Using Authority column: {auth_col}\")\n",
    "\n",
    "    # 3. PREPARE CATEGORIES\n",
    "    # Create rich path for matching: \"Level 1 > Level 2 > Level 3\"\n",
    "    cat_paths = df_cats.apply(lambda x: ' > '.join(x.dropna().astype(str)), axis=1).tolist()\n",
    "    \n",
    "    # Identify clean target name for the final file\n",
    "    if 'Category (en)' in df_cats.columns:\n",
    "        target_labels = df_cats['Category (en)'].tolist()\n",
    "    else:\n",
    "        target_labels = cat_paths\n",
    "\n",
    "    # 4. ENCODE & MATCH\n",
    "    print(\"Loading model...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    print(\"Encoding data (this may take a moment)...\")\n",
    "    cleaned_names = [clean_text(str(x)) for x in df_prods[prod_col]]\n",
    "    \n",
    "    # Encode\n",
    "    prod_emb = model.encode(cleaned_names, convert_to_tensor=True)\n",
    "    cat_emb = model.encode(cat_paths, convert_to_tensor=True)\n",
    "\n",
    "    print(\"Matching...\")\n",
    "    cosine_scores = util.cos_sim(prod_emb, cat_emb)\n",
    "    top_scores, top_indices = cosine_scores.max(dim=1)\n",
    "\n",
    "    # 5. BUILD RESULTS\n",
    "    results = []\n",
    "    for i, name in enumerate(df_prods[prod_col]):\n",
    "        score = top_scores[i].item()\n",
    "        cat_idx = top_indices[i].item()\n",
    "        \n",
    "        row_data = {\n",
    "            \"Original Product\": name,\n",
    "            \"Matched Category\": target_labels[cat_idx], # Clean single column\n",
    "            \"Matched Path\": cat_paths[cat_idx],         # Full path (optional, helpful for review)\n",
    "            \"Confidence\": round(score, 4),\n",
    "            \"Status\": \"Review Needed\" if score < CONFIDENCE_THRESHOLD else \"Auto-Assigned\"\n",
    "        }\n",
    "        if auth_col:\n",
    "            row_data['Registration Authority'] = df_prods.iloc[i][auth_col]\n",
    "            \n",
    "        results.append(row_data)\n",
    "\n",
    "    df_res = pd.DataFrame(results)\n",
    "\n",
    "    # 6. STATISTICS & COUNTS\n",
    "    if auth_col:\n",
    "        print(\"\\n--- Generating Statistics ---\")\n",
    "        \n",
    "        # A. Unique Categories per Operator\n",
    "        auth_unique = df_res.groupby('Registration Authority')['Matched Category'].nunique().reset_index()\n",
    "        auth_unique.columns = ['Registration Authority', 'Unique Categories']\n",
    "        auth_unique.to_excel(STATS_FILE, index=False)\n",
    "        \n",
    "        # B. Detailed Counts (Number of EPDs per Category per Operator)\n",
    "        # Group by Authority AND Category, then count rows\n",
    "        cat_counts = df_res.groupby(['Registration Authority', 'Matched Category']).size().reset_index(name='Number of EPDs')\n",
    "        cat_counts = cat_counts.sort_values(['Registration Authority', 'Number of EPDs'], ascending=[True, False])\n",
    "        \n",
    "        cat_counts.to_excel(COUNTS_FILE, index=False)\n",
    "        print(f\"Detailed counts saved to: {COUNTS_FILE}\")\n",
    "\n",
    "    # 7. SAVE MAIN RESULTS\n",
    "    df_res.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(f\"Main results saved to: {OUTPUT_FILE}\")\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20ab5ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-Assigned by Registration Authority:\n",
      "Registration_Authority_Clean  Number of Auto-Assigned EPDs\n",
      "  AENOR Internacional S.A.U.                             1\n",
      "           DAPHabitat System                            39\n",
      "                 EPD Denmark                           711\n",
      "                 EPD Ireland                            96\n",
      "                  EPD Norway                            31\n",
      "               EPDFinlandRTS                            35\n",
      "                    EPDItaly                          1045\n",
      "                 ICMQ S.p.A.                             7\n",
      "              ITB-EPD Poland                           157\n",
      "    Programa DAPconstrucción                           104\n",
      "             Stichting MRPI®                           322\n",
      "            Stichting MRPIÂ®                            13\n",
      "                     Unknown                           335\n",
      "\n",
      "Review Needed by Registration Authority:\n",
      "Registration_Authority_Clean  Number of Review Needed EPDs\n",
      "           DAPHabitat System                            17\n",
      "                 EPD Denmark                           295\n",
      "                 EPD Ireland                            55\n",
      "                  EPD Norway                            22\n",
      "               EPDFinlandRTS                            41\n",
      "                    EPDItaly                           299\n",
      "                 ICMQ S.p.A.                             4\n",
      "              ITB-EPD Poland                            85\n",
      "    Programa DAPconstrucción                            47\n",
      "             Stichting MRPI®                           160\n",
      "            Stichting MRPIÂ®                             6\n",
      "                     Unknown                           259\n",
      "\n",
      "EPDs without a program operator (missing/unknown): 594\n",
      "       Status  Number of EPDs\n",
      "Auto-Assigned             335\n",
      "Review Needed             259\n",
      "Saved mapping_missing_operator_counts.xlsx\n"
     ]
    }
   ],
   "source": [
    "# number of EPDs assigned product category grouped by program operator\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "FILE = 'Mapped_EPD_Results.xlsx'\n",
    "if not os.path.exists(FILE):\n",
    "    raise FileNotFoundError(f'{FILE} not found. Run the mapper to produce mapping results.')\n",
    "df1 = pd.read_excel(FILE)\n",
    "\n",
    "# Detect registration authority column name (case-insensitive)\n",
    "reg_col = None\n",
    "for c in df1.columns:\n",
    "    if c.lower().strip() == 'registration authority':\n",
    "        reg_col = c\n",
    "        break\n",
    "if reg_col is None:\n",
    "    for c in df1.columns:\n",
    "        if 'registration' in c.lower() and 'authority' in c.lower():\n",
    "            reg_col = c\n",
    "            break\n",
    "if reg_col is None:\n",
    "    # fallback to a canonical name and create it if missing\n",
    "    reg_col = 'Registration Authority'\n",
    "    df1[reg_col] = pd.NA\n",
    "\n",
    "# Normalize a clean column for grouping\n",
    "df1['Registration_Authority_Clean'] = df1[reg_col].astype(str).str.strip()\n",
    "# Treat empty strings and common placeholders as missing\n",
    "df1['Registration_Authority_Clean'] = df1['Registration_Authority_Clean'].replace({'': pd.NA, 'nan': pd.NA})\n",
    "df1['Registration_Authority_Clean'] = df1['Registration_Authority_Clean'].fillna('Unknown')\n",
    "\n",
    "# Counts by operator and status\n",
    "assigned_counts = df1[df1['Status'] == 'Auto-Assigned'].groupby('Registration_Authority_Clean').size().reset_index(name='Number of Auto-Assigned EPDs')\n",
    "review_counts = df1[df1['Status'] == 'Review Needed'].groupby('Registration_Authority_Clean').size().reset_index(name='Number of Review Needed EPDs')\n",
    "\n",
    "print('Auto-Assigned by Registration Authority:')\n",
    "print(assigned_counts.head(50).to_string(index=False))\n",
    "\n",
    "print('\\nReview Needed by Registration Authority:')\n",
    "print(review_counts.head(50).to_string(index=False))\n",
    "\n",
    "# Counts where registration authority is missing/unknown\n",
    "missing_mask = df1[reg_col].isna() | df1[reg_col].astype(str).str.strip().eq('') | df1[reg_col].astype(str).str.lower().eq('unknown')\n",
    "missing_total = missing_mask.sum()\n",
    "print(f'\\nEPDs without a program operator (missing/unknown): {missing_total}')\n",
    "missing_by_status = df1[missing_mask].groupby('Status').size().reindex(['Auto-Assigned','Review Needed']).fillna(0).astype(int).reset_index(name='Number of EPDs')\n",
    "print(missing_by_status.to_string(index=False))\n",
    "\n",
    "# Save a small Excel summary for missing operators\n",
    "missing_by_status.to_excel('mapping_missing_operator_counts.xlsx', index=False)\n",
    "print('Saved mapping_missing_operator_counts.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (brightway)",
   "language": "python",
   "name": "brightway"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
