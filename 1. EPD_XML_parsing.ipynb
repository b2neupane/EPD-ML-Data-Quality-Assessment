{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "732d2325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Defaulting to user installation because normal site-packages is not writeable',\n",
       " 'Collecting deep-translator',\n",
       " '  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)',\n",
       " 'Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages (from deep-translator) (4.12.3)',\n",
       " 'Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages (from deep-translator) (2.32.3)',\n",
       " 'Requirement already satisfied: soupsieve>1.2 in c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)',\n",
       " 'Requirement already satisfied: charset-normalizer<4,>=2 in c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)',\n",
       " 'Requirement already satisfied: idna<4,>=2.5 in c:\\\\users\\\\user\\\\appdata\\\\roaming\\\\python\\\\python312\\\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)',\n",
       " 'Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.2.3)',\n",
       " 'Requirement already satisfied: certifi>=2017.4.17 in c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.8.30)',\n",
       " 'Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)',\n",
       " 'Installing collected packages: deep-translator',\n",
       " \"  WARNING: The scripts deep-translator.exe and dt.exe are installed in 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\Scripts' which is not on PATH.\",\n",
       " '  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.',\n",
       " 'Successfully installed deep-translator-1.11.4',\n",
       " '',\n",
       " '[notice] A new release of pip is available: 25.0 -> 25.0.1',\n",
       " '[notice] To update, run: python.exe -m pip install --upgrade pip']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "!! pip install -U deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac948b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished! Total EPDs parsed and written to CSV: 22398\n",
      "‚ùå Failed EPDs: 1\n",
      "üìù Failure log written to: epd_failures.log\n",
      "üìÑ Output file saved as: output.csv\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import csv\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Input and output file paths\n",
    "input_path = \"epd_data_XML_format.xml\"  \n",
    "output_path = \"output.csv\"\n",
    "\n",
    "# CSV header columns \n",
    "header = [\n",
    "    \"UUID\", \"Version\", \"Name (original)\", \"Name (en)\", \n",
    "    \"Category (original)\", \"Category (en)\", \"Compliance\", \"Location code\", \"Type\",\n",
    "    \"Reference year\", \"Valid until\", \"URL\", \"Declaration owner\", \"Publication date\",\n",
    "    \"Registration number\", \"Registration authority\", \"Predecessor UUID\", \"Predecessor Version\",\n",
    "    \"Predecessor URL\", \"Ref. quantity\", \"Ref. unit\", \"Reference flow UUID\", \"Reference flow name\",\n",
    "    \"Bulk Density (kg/m3)\", \"Grammage (kg/m2)\", \"Gross Density (kg/m3)\", \"Layer Thickness (m)\",\n",
    "    \"Productiveness (m2)\", \"Linear Density (kg/m)\", \"Weight Per Piece (kg)\", \"Conversion factor to 1kg\",\n",
    "    \"Carbon content (biogenic) in kg\", \"Carbon content (biogenic) - packaging in kg\",\n",
    "    \"Module\", \"Scenario\", \"Scenario Description\",\n",
    "    \"GWP\", \"ODP\", \"POCP\", \"AP\", \"EP\", \"ADPE\", \"ADPF\",\n",
    "    \"PERE\", \"PERM\", \"PERT\", \"PENRE\", \"PENRM\", \"PENRT\",\n",
    "    \"SM\", \"RSF\", \"NRSF\", \"FW\", \"HWD\", \"NHWD\", \"RWD\",\n",
    "    \"CRU\", \"MFR\", \"MER\", \"EEE\", \"EET\",\n",
    "    \"AP (A2)\", \"GWPtotal (A2)\", \"GWPbiogenic (A2)\", \"GWPfossil (A2)\", \"GWPluluc (A2)\",\n",
    "    \"ETPfw (A2)\", \"PM (A2)\", \"EPmarine (A2)\", \"EPfreshwater (A2)\", \"EPterrestrial (A2)\",\n",
    "    \"HTPc (A2)\", \"HTPnc (A2)\", \"IRP (A2)\", \"SOP (A2)\", \"ODP (A2)\", \"POCP (A2)\",\n",
    "    \"ADPF (A2)\", \"ADPE (A2)\", \"WDP (A2)\", \"GWP_IOBC_GHG\", \"\"\n",
    "]\n",
    "# index lookup for columns\n",
    "col_index = {col: idx for idx, col in enumerate(header)}\n",
    "\n",
    "# Mappings for indicator acronyms to CSV column names\n",
    "# Older EN15804+A1 indicators:\n",
    "indicator_map_A1 = {\n",
    "    \"GWP\": \"GWP\", \"ODP\": \"ODP\", \"POCP\": \"POCP\", \"AP\": \"AP\",\n",
    "    \"EP\": \"EP\", \"ADPE\": \"ADPE\", \"ADPF\": \"ADPF\"\n",
    "}\n",
    "# New EN15804+A2 indicators (including split categories and new metrics):\n",
    "indicator_map_A2 = {\n",
    "    \"AP\": \"AP (A2)\",\n",
    "    \"GWP-total\": \"GWPtotal (A2)\",\n",
    "    \"GWP-biogenic\": \"GWPbiogenic (A2)\",\n",
    "    \"GWP-fossil\": \"GWPfossil (A2)\",\n",
    "    \"GWP-luluc\": \"GWPluluc (A2)\",\n",
    "    \"ETP-fw\": \"ETPfw (A2)\",\n",
    "    \"PM\": \"PM (A2)\",\n",
    "    \"EP-marine\": \"EPmarine (A2)\",\n",
    "    \"EP-freshwater\": \"EPfreshwater (A2)\",\n",
    "    \"EP-terrestrial\": \"EPterrestrial (A2)\",\n",
    "    \"HTP-c\": \"HTPc (A2)\",\n",
    "    \"HTP-nc\": \"HTPnc (A2)\",\n",
    "    \"IRP\": \"IRP (A2)\",\n",
    "    \"ODP\": \"ODP (A2)\",\n",
    "    \"POCP\": \"POCP (A2)\",\n",
    "    \"ADPF\": \"ADPF (A2)\",\n",
    "    \"ADPE\": \"ADPE (A2)\",\n",
    "    \"WDP\": \"WDP (A2)\",\n",
    "    \"SQP\": \"SOP (A2)\",            # Soil Quality Potential (land use) -> SOP (A2)\n",
    "    \"GWP-IOBC/GHG\": \"GWP_IOBC_GHG\"  # Biogenic carbon uptake/emission -> GWP_IOBC_GHG\n",
    "}\n",
    "# Abbreviations for resource use and waste flows to column names (they match 1:1)\n",
    "flow_map = {abbr: abbr for abbr in [\n",
    "    \"PERE\", \"PERM\", \"PERT\", \"PENRE\", \"PENRM\", \"PENRT\",\n",
    "    \"SM\", \"RSF\", \"NRSF\", \"FW\", \"HWD\", \"NHWD\", \"RWD\",\n",
    "    \"CRU\", \"MFR\", \"MER\", \"EEE\", \"EET\"\n",
    "]}\n",
    "\n",
    "# Helper function: parse one <processDataSet> XML string and return list of CSV rows (as lists of values)\n",
    "def parse_epd_xml(xml_string):\n",
    "    root = ET.fromstring(xml_string)  # parse the EPD XML chunk\n",
    "    \n",
    "    # Base values (common to all module rows of this EPD)\n",
    "    base = [\"\"] * len(header)\n",
    "    # Basic dataset metadata\n",
    "    base[col_index[\"UUID\"]] = (root.findtext('.//{http://lca.jrc.it/ILCD/Common}UUID') or \"\").strip()\n",
    "    base[col_index[\"Version\"]] = (root.findtext('.//{http://lca.jrc.it/ILCD/Common}dataSetVersion') or \"\").strip()\n",
    "    \n",
    "    # Initialize translator for English translation\n",
    "    translator = GoogleTranslator(source='auto', target='en')\n",
    "    # Initialize name variables\n",
    "    name_original = \"\"\n",
    "    name_en = \"\"\n",
    "    # Name in multiple languages\n",
    "    name_elem = root.find('.//{http://lca.jrc.it/ILCD/Process}name')\n",
    "    if name_elem is not None:\n",
    "        for bn in name_elem.findall('{http://lca.jrc.it/ILCD/Process}baseName'):\n",
    "            lang = bn.get('{http://www.w3.org/XML/1998/namespace}lang', \"\")\n",
    "            name_text = (bn.text or \"\").strip() \n",
    "            if not name_original:\n",
    "                name_original = name_text\n",
    "            if lang == \"en\" and not name_en:\n",
    "                name_en = name_text   \n",
    "    if not name_en and name_original:\n",
    "        try:\n",
    "            name_en = translator.translate(name_original)\n",
    "        except Exception as e:\n",
    "            print(f\"Translation failed for: {name_original} ‚Üí {e}\")\n",
    "            name_en = name_original  # fallback to original if translation fails\n",
    "    \n",
    "    base[col_index[\"Name (original)\"]] = name_original\n",
    "    base[col_index[\"Name (en)\"]] = name_en\n",
    "\n",
    "    # Category classification (original and translated)\n",
    "    class_info = root.find('.//{http://lca.jrc.it/ILCD/Process}classificationInformation')\n",
    "    if class_info is not None:\n",
    "        class_node = class_info.find('{http://lca.jrc.it/ILCD/Common}classification')\n",
    "        if class_node is not None:\n",
    "            classes = [cls.text or \"\" for cls in class_node.findall('{http://lca.jrc.it/ILCD/Common}class')]\n",
    "            if classes:\n",
    "                # Original categories with quotes\n",
    "                orig_cats = \" / \".join(f\"'{c}'\" for c in classes)\n",
    "                base[col_index[\"Category (original)\"]] = orig_cats\n",
    "                # English categories (translate if known, else same as original)\n",
    "                if class_node.get('name') == 'EPDNorge':\n",
    "                    # Known translation mapping for EPD Norge categories:\n",
    "                    trans = []\n",
    "                    for c in classes:\n",
    "                        if c == \"Bygg\":\n",
    "                            trans.append(\"'Construction'\")\n",
    "                        elif \"St√•l\" in c:\n",
    "                            trans.append(\"'Steel and Aluminium'\")\n",
    "                        else:\n",
    "                            trans.append(f\"'{c}'\")\n",
    "                    base[col_index[\"Category (en)\"]] = \" / \".join(trans)\n",
    "                else:\n",
    "                    base[col_index[\"Category (en)\"]] = orig_cats\n",
    "\n",
    "    \n",
    "    # Compliance standards\n",
    "    compliance_texts = []\n",
    "    for comp in root.findall('.//{http://lca.jrc.it/ILCD/Common}referenceToComplianceSystem'):\n",
    "        desc = comp.find('{http://lca.jrc.it/ILCD/Common}shortDescription')\n",
    "        if desc is not None and desc.text:\n",
    "            compliance_texts.append(f\"'{desc.text}'\")\n",
    "    base[col_index[\"Compliance\"]] = \" / \".join(compliance_texts)\n",
    "\n",
    "    # Geography/location\n",
    "    loc_elem = root.find('.//{http://lca.jrc.it/ILCD/Process}locationOfOperationSupplyOrProduction')\n",
    "    if loc_elem is not None:\n",
    "        base[col_index[\"Location code\"]] = loc_elem.get('location', \"\")\n",
    "\n",
    "    # Dataset type (EPD subtype)\n",
    "    subtype = root.find('.//{http://www.iai.kit.edu/EPD/2013}subType')\n",
    "    if subtype is not None:\n",
    "        base[col_index[\"Type\"]] = (subtype.text or \"\").strip()\n",
    "    \n",
    "    # Time (reference year and valid until)\n",
    "    year = root.findtext('.//{http://lca.jrc.it/ILCD/Common}referenceYear')\n",
    "    if year:\n",
    "        base[col_index[\"Reference year\"]] = year.strip()\n",
    "    valid_until = root.findtext('.//{http://lca.jrc.it/ILCD/Common}dataSetValidUntil')\n",
    "    if valid_until:\n",
    "        base[col_index[\"Valid until\"]] = valid_until.strip()[:4]\n",
    "    \n",
    "    # URL (constructed from UUID and version)\n",
    "    if base[col_index[\"UUID\"]] and base[col_index[\"Version\"]]:\n",
    "        base[col_index[\"URL\"]] = f\"processes/{base[col_index['UUID']]}?version={base[col_index['Version']]}\"\n",
    "    \n",
    "    # Declaration owner (organization)\n",
    "    owner_node = root.find('.//{http://lca.jrc.it/ILCD/Common}referenceToOwnershipOfDataSet')\n",
    "    if owner_node is not None:\n",
    "        owner_name = owner_node.find('{http://lca.jrc.it/ILCD/Common}shortDescription')\n",
    "        if owner_name is not None:\n",
    "            base[col_index[\"Declaration owner\"]] = owner_name.text or \"\"\n",
    "    \n",
    "    # Publication date of EPD\n",
    "    pub_date = root.findtext('.//{http://www.indata.network/EPD/2019}publicationDateOfEPD')\n",
    "    if pub_date:\n",
    "        base[col_index[\"Publication date\"]] = pub_date\n",
    "    \n",
    "    # Registration number and authority\n",
    "    base[col_index[\"Registration number\"]] = root.findtext('.//{http://lca.jrc.it/ILCD/Common}registrationNumber') or \"\"\n",
    "    reg_auth_node = root.find('.//{http://lca.jrc.it/ILCD/Common}referenceToRegistrationAuthority')\n",
    "    if reg_auth_node is not None:\n",
    "        auth_name = reg_auth_node.find('{http://lca.jrc.it/ILCD/Common}shortDescription')\n",
    "        if auth_name is not None:\n",
    "            base[col_index[\"Registration authority\"]] = auth_name.text or \"\"\n",
    "    \n",
    "    # Predecessor EPD reference (if any)\n",
    "    prev_node = root.find('.//{http://www.indata.network/EPD/2019}referenceToPreviousEPD')\n",
    "    if prev_node is not None:\n",
    "        base[col_index[\"Predecessor UUID\"]]   = prev_node.get('refObjectId', \"\")\n",
    "        base[col_index[\"Predecessor Version\"]] = prev_node.get('version', \"\")\n",
    "        base[col_index[\"Predecessor URL\"]]    = prev_node.get('uri', \"\")\n",
    "    \n",
    "    # Reference flow (declared unit) details\n",
    "    ref_flow_id = root.findtext('.//{http://lca.jrc.it/ILCD/Process}referenceToReferenceFlow')\n",
    "    if ref_flow_id:\n",
    "        # Find the exchange entry with this internal ID\n",
    "        for exch in root.findall('.//{http://lca.jrc.it/ILCD/Process}exchange'):\n",
    "            if exch.get('dataSetInternalID') == ref_flow_id.strip():\n",
    "                # Reference flow UUID and name\n",
    "                flow = exch.find('{http://lca.jrc.it/ILCD/Process}referenceToFlowDataSet')\n",
    "                if flow is not None:\n",
    "                    base[col_index[\"Reference flow UUID\"]] = flow.get('refObjectId', \"\")\n",
    "                    # Choose English name if available\n",
    "                    names = flow.findall('{http://lca.jrc.it/ILCD/Common}shortDescription')\n",
    "                    ref_name = \"\"\n",
    "                    for nm in names:\n",
    "                        if nm.get('{http://www.w3.org/XML/1998/namespace}lang') == 'en':\n",
    "                            ref_name = nm.text or \"\"\n",
    "                            break\n",
    "                        if not ref_name:\n",
    "                            ref_name = nm.text or \"\"\n",
    "                    base[col_index[\"Reference flow name\"]] = ref_name.strip()\n",
    "                # Reference flow amount and unit\n",
    "                base[col_index[\"Ref. quantity\"]] = exch.findtext('{http://lca.jrc.it/ILCD/Process}meanAmount') or \"\"\n",
    "                unit_ref = exch.find('.//{http://www.iai.kit.edu/EPD/2013}referenceToUnitGroupDataSet')\n",
    "                if unit_ref is not None:\n",
    "                    unit_sd = unit_ref.find('{http://lca.jrc.it/ILCD/Common}shortDescription')\n",
    "                    if unit_sd is not None:\n",
    "                        base[col_index[\"Ref. unit\"]] = unit_sd.text or \"\"\n",
    "                if not base[col_index[\"Ref. unit\"]] and base[col_index[\"Reference flow name\"]]:\n",
    "                    # Derive unit from reference flow name if not explicitly given\n",
    "                    match = re.search(r'1\\s+(\\w+)$', base[col_index[\"Reference flow name\"]])\n",
    "                    if match:\n",
    "                        base[col_index[\"Ref. unit\"]] = match.group(1)\n",
    "                break\n",
    "    # Default missing biogenic carbon content to \"0\" as in sample\n",
    "    if not base[col_index[\"Carbon content (biogenic) in kg\"]]:\n",
    "        base[col_index[\"Carbon content (biogenic) in kg\"]] = \"0\"\n",
    "    if not base[col_index[\"Carbon content (biogenic) - packaging in kg\"]]:\n",
    "        base[col_index[\"Carbon content (biogenic) - packaging in kg\"]] = \"0\"\n",
    "\n",
    "    # Determine which modules (life-cycle stages) are present\n",
    "    module_values = {}  # track sum of values per module to identify empty ones\n",
    "    for amt in root.findall('.//{http://www.iai.kit.edu/EPD/2013}amount'):\n",
    "        mod = amt.get('{http://www.iai.kit.edu/EPD/2013}module')\n",
    "        if mod:\n",
    "            # Initialize tracking\n",
    "            if mod not in module_values:\n",
    "                module_values[mod] = 0.0\n",
    "            # Sum numeric values (non-numeric treated as 0)\n",
    "            try:\n",
    "                module_values[mod] += float(amt.text)\n",
    "            except:\n",
    "                module_values[mod] += 0.0\n",
    "\n",
    "    # Prepare output rows for each relevant module\n",
    "    module_order = [\"A1-A3\", \"A4\", \"A5\", \"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"C1\", \"C2\", \"C3\", \"C4\", \"D\"]\n",
    "    rows = []\n",
    "    for mod in module_order:\n",
    "        if mod in module_values:\n",
    "            # Skip B modules if all values are zero (not reported in sample)\n",
    "            if mod.startswith(\"B\") and abs(module_values.get(mod, 0.0)) < 1e-12:\n",
    "                continue\n",
    "            row = base.copy()\n",
    "            row[col_index[\"Module\"]] = mod\n",
    "            rows.append(row)\n",
    "    if not rows:\n",
    "        return rows\n",
    "\n",
    "    # Determine EN15804 standard version (A2 or A1) for indicator mapping\n",
    "    is_A2 = \"15804+A2\" in base[col_index[\"Compliance\"]]\n",
    "\n",
    "    # Fill resource use and waste indicators from exchanges\n",
    "    for exch in root.findall('.//{http://lca.jrc.it/ILCD/Process}exchange'):\n",
    "        # Get flow short description and abbreviation in parentheses\n",
    "        flow_node = exch.find('{http://lca.jrc.it/ILCD/Process}referenceToFlowDataSet')\n",
    "        if flow_node is None:\n",
    "            continue\n",
    "        desc_node = flow_node.find('{http://lca.jrc.it/ILCD/Common}shortDescription')\n",
    "        if desc_node is None or not desc_node.text:\n",
    "            continue\n",
    "        match = re.search(r'\\(([^)]+)\\)$', desc_node.text)\n",
    "        if not match:\n",
    "            continue\n",
    "        abbrev = match.group(1)\n",
    "        if abbrev in flow_map:\n",
    "            # Assign this flow's values to each module\n",
    "            for amt in exch.findall('.//{http://www.iai.kit.edu/EPD/2013}amount'):\n",
    "                mod = amt.get('{http://www.iai.kit.edu/EPD/2013}module')\n",
    "                if mod in module_values:  # module exists\n",
    "                    # Find the corresponding row for this module\n",
    "                    for row in rows:\n",
    "                        if row[col_index[\"Module\"]] == mod:\n",
    "                            row[col_index[flow_map[abbrev]]] = amt.text or \"\"\n",
    "\n",
    "    # Fill LCIA results (impact indicators) from LCIAResult entries\n",
    "    for lcia in root.findall('.//{http://lca.jrc.it/ILCD/Process}LCIAResult'):\n",
    "        desc = lcia.find('.//{http://lca.jrc.it/ILCD/Common}shortDescription')\n",
    "        if desc is None or not desc.text:\n",
    "            continue\n",
    "        match = re.search(r'\\(([^)]+)\\)$', desc.text)\n",
    "        if not match:\n",
    "            continue\n",
    "        code = match.group(1).strip()\n",
    "        # Determine which mapping to use for this code\n",
    "        if is_A2:\n",
    "            # A2 standard: prefer new map, fall back to old if not in new\n",
    "            if code in indicator_map_A2:\n",
    "                col_name = indicator_map_A2[code]\n",
    "            elif code in indicator_map_A1 and code not in indicator_map_A2:\n",
    "                col_name = indicator_map_A1[code]\n",
    "            else:\n",
    "                # Try removing special chars (e.g., hyphens or slashes) to match keys\n",
    "                code_clean = code.replace(\"-\", \"\").replace(\"/\", \"\")\n",
    "                if code_clean in indicator_map_A2:\n",
    "                    col_name = indicator_map_A2[code_clean]\n",
    "                else:\n",
    "                    continue\n",
    "        else:\n",
    "            # A1 standard: use old map\n",
    "            if code in indicator_map_A1:\n",
    "                col_name = indicator_map_A1[code]\n",
    "            elif code in indicator_map_A2 and code not in indicator_map_A1:\n",
    "                col_name = indicator_map_A2[code]  # (Just in case, e.g., if an A2 indicator appears in an A1 dataset)\n",
    "            else:\n",
    "                code_clean = code.replace(\"-\", \"\").replace(\"/\", \"\")\n",
    "                if code_clean in indicator_map_A1:\n",
    "                    col_name = indicator_map_A1[code_clean]\n",
    "                else:\n",
    "                    continue\n",
    "        # Assign indicator values to each module row\n",
    "        for amt in lcia.findall('.//{http://www.iai.kit.edu/EPD/2013}amount'):\n",
    "            mod = amt.get('{http://www.iai.kit.edu/EPD/2013}module')\n",
    "            if not mod:\n",
    "                continue\n",
    "            for row in rows:\n",
    "                if row[col_index[\"Module\"]] == mod:\n",
    "                    row[col_index[col_name]] = amt.text or \"\"\n",
    "    return rows\n",
    "\n",
    "# Read the input file and write to CSV\n",
    "with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter=';', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(header)\n",
    "    buffer = []  # to accumulate lines of one EPD\n",
    "    \n",
    "    epd_count = 0  # count of processed EPDs\n",
    "    failed_count = 0  # count of failed EPDs\n",
    "    log_path = 'epd_failures.log'\n",
    "    log_lines = []  # to accumulate lines of failed EPDs\n",
    "    line_num = 0  # line number in the input file\n",
    "    epd_start_line = 0  # line number of the start of the current EPD\n",
    "    for line in infile:\n",
    "        line_num += 1\n",
    "        if line.strip().startswith(\"<?xml\"):\n",
    "            if buffer:\n",
    "                try:\n",
    "                    epd_rows = parse_epd_xml(\"\".join(buffer))\n",
    "                    if epd_rows:\n",
    "                        epd_count += 1\n",
    "                except Exception as e:\n",
    "                    failed_count += 1\n",
    "                    buffer_str = \"\".join(buffer)\n",
    "                    uuid_match = re.search(r\"<UUID>(.*?)</UUID>\", buffer_str)\n",
    "                    uuid = uuid_match.group(1).strip() if uuid_match else \"UNKNOWN\"\n",
    "                    log_lines.append(f\"Failed EPD at line {epd_start_line}, UUID={uuid}: {str(e)}\")\n",
    "                for row in epd_rows:\n",
    "                    writer.writerow(row)\n",
    "            buffer = [line]\n",
    "            epd_start_line = line_num\n",
    "        else:\n",
    "            if buffer:\n",
    "                buffer.append(line)\n",
    "\n",
    "    # Handle final buffer\n",
    "    if buffer:\n",
    "        try:\n",
    "            epd_rows = parse_epd_xml(\"\".join(buffer))\n",
    "            if epd_rows:\n",
    "                epd_count += 1\n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            buffer_str = \"\".join(buffer)\n",
    "            uuid_match = re.search(r\"<UUID>(.*?)</UUID>\", buffer_str)\n",
    "            uuid = uuid_match.group(1).strip() if uuid_match else \"UNKNOWN\"\n",
    "            log_lines.append(f\"Failed final EPD at line {epd_start_line}, UUID={uuid}: {str(e)}\")\n",
    "            epd_rows = []\n",
    "        for row in epd_rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    " # Write failure log\n",
    "if log_lines:\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as log_file:\n",
    "        log_file.write(\"\\n\".join(log_lines))\n",
    "\n",
    "print(f\"‚úÖ Finished! Total EPDs parsed and written to CSV: {epd_count}\")\n",
    "print(f\"‚ùå Failed EPDs: {failed_count}\")\n",
    "if failed_count > 0:\n",
    "    print(f\"üìù Failure log written to: {log_path}\")\n",
    "print(f\"üìÑ Output file saved as: {output_path}\")   \n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
